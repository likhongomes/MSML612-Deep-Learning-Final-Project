# -*- coding: utf-8 -*-
"""Tabnet_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13TIm4vU2i_q_BUaizPdaIJqe1kaGf23A
"""

# ============================================================
# MSML 612 – Parking Violations Forecasting (Daily, DC)
# Unified pipeline: Features + Baselines + GBDT + SARIMAX + TabNet
# Outputs -> /content/drive/MyDrive/MSML612/Output_tabnet_ateeq
# ============================================================

# ----------------------------
# 0) Colab: Mount Google Drive
# ----------------------------
COLAB = True
if COLAB:
    from google.colab import drive
    drive.mount('/content/drive')

# ============================================================
# MSML 612 – DC Parking Violations (Daily)
# Baselines + SARIMAX + TabNet (per-horizon & multi-task)
# + Conformal + Rolling CV + TabTransformer + LSTM
# + Inline table display + HTML exports + Leaderboard charts
# + ALL-MODELS overlay per horizon
# Auto-selects CPU/GPU for PyTorch & TensorFlow
# Outputs -> /content/drive/MyDrive/MSML612 Project/Output_tabnet_ateeq
# ============================================================

# 0.1) Ensure required packages (lightweight guards)
import sys, subprocess
def _pip_install(pkg):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg, "-q"])
    except Exception as e:
        print(f"[WARN] pip install for {pkg} failed:", e)

# Try TabNet; install if missing
try:
    from pytorch_tabnet.tab_model import TabNetRegressor
except Exception:
    _pip_install("pytorch-tabnet")
    from pytorch_tabnet.tab_model import TabNetRegressor

# Try TensorFlow; install CPU build if missing
try:
    import tensorflow as tf
except Exception:
    _pip_install("tensorflow-cpu")
    import tensorflow as tf

# --------------------
# 1) Imports & Config
# --------------------
from pathlib import Path
import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt
from datetime import timedelta

from sklearn.linear_model import LinearRegression, LassoCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.base import clone

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tools.sm_exceptions import ValueWarning

import torch
from tensorflow import keras
from tensorflow.keras import layers
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

warnings.filterwarnings("ignore", category=ValueWarning)
np.random.seed(42); torch.manual_seed(42); tf.random.set_seed(42)

# ---- Device autodetect (PyTorch + TensorFlow) ----
USE_CUDA = torch.cuda.is_available()
DEVICE_NAME = "cuda" if USE_CUDA else "cpu"
print("PyTorch device:", DEVICE_NAME)

gpus = tf.config.list_physical_devices("GPU")
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"TensorFlow device: GPU x{len(gpus)} (memory growth enabled)")
    except Exception as e:
        print("TensorFlow GPU present but couldn't set memory growth:", e)
        print("TensorFlow will still try to use the GPU.")
else:
    print("TensorFlow device: CPU")

# ---- Pretty tables (display + save HTML) ----
pd.set_option("display.width", 180)
pd.set_option("display.max_columns", 200)
pd.set_option("display.max_rows", 200)

def show_table(df, title):
    print("\n" + "="*len(title))
    print(title)
    print("="*len(title))
    try:
        from IPython.display import display
        display(df)
    except Exception:
        print(df.to_string())

def save_html(df, path):
    df.to_html(path, index=True if df.index.name or isinstance(df.index, pd.MultiIndex) else False)

# ---- Paths (canonical output folder) ----
DATA_PATH   = Path("/content/drive/MyDrive/MSML612 Project/Data")
OUTPUT_PATH = Path("/content/drive/MyDrive/MSML612 Project/Output_tabnet_ateeq")
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
print("All artifacts will be saved to:", OUTPUT_PATH)

# ---- Canonical DAILY CSV (same source for ALL models) ----
CANDIDATE_DAILY_FILES = [
    DATA_PATH / "cleaned_parking_violations_v3_daily_aggregate.csv",
    DATA_PATH / "daily_timeseries_parking_violations_v2.csv",
]
def choose_daily_source(candidates):
    for p in candidates:
        if p.exists(): return p
    raise FileNotFoundError("No daily CSV found. Expected one of: " + ", ".join(str(x) for x in candidates))
DAILY_FILE = choose_daily_source(CANDIDATE_DAILY_FILES)
print("Using DAILY CSV for ALL models →", DAILY_FILE)

# --------------------
# 2) Global Settings
# --------------------
TARGET    = "num_violations"
DATE_COL  = "date"
HORIZONS  = [1, 3, 7]

TRAIN_END = pd.Timestamp("2024-12-31")
VAL_END   = pd.Timestamp("2025-03-31")  # test > this

# ----------------------
# 3) Utility Functions
# ----------------------
def load_and_patch_daily(path: Path, date_col=DATE_COL, target_col=TARGET) -> pd.DataFrame:
    df = pd.read_csv(path, parse_dates=[date_col])
    # continuous daily index
    full_index = pd.date_range(df[date_col].min(), df[date_col].max(), freq="D")
    df = df.set_index(date_col).reindex(full_index)
    df.index.name = date_col
    df.reset_index(inplace=True)

    # fill numerics (except date)
    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != date_col]
    df[num_cols] = df[num_cols].fillna(0)

    # calendar basics
    df["year"]        = df[date_col].dt.year
    df["month"]       = df[date_col].dt.month
    df["day_of_week"] = df[date_col].dt.day_name()
    df["is_weekend"]  = (df[date_col].dt.dayofweek >= 5).astype("int8")

    if "is_holiday" in df.columns:
        df["is_holiday"] = pd.to_numeric(df["is_holiday"], errors="coerce").fillna(0).astype("int8")
    else:
        df["is_holiday"] = 0

    return df

def add_temporal_features(df: pd.DataFrame,
                          date_col=DATE_COL,
                          target_col=TARGET) -> pd.DataFrame:
    df = df.copy().sort_values(date_col)

    # lags
    for lag in [1, 7, 14, 30]:
        df[f"{target_col}_lag_{lag}"] = df[target_col].shift(lag)

    # rolling stats
    df[f"{target_col}_roll7_mean"]  = df[target_col].rolling(7).mean()
    df[f"{target_col}_roll30_mean"] = df[target_col].rolling(30).mean()
    df[f"{target_col}_roll30_std"]  = df[target_col].rolling(30).std()

    # relative deviation vs 7-day mean
    df["roc_vs_roll7"] = (df[target_col] - df[f"{target_col}_roll7_mean"]) / (df[f"{target_col}_roll7_mean"] + 1e-6)

    # Fourier seasonality (annual)
    doy = df[date_col].dt.dayofyear
    for k in [1, 2]:
        df[f"sin_{k}"] = np.sin(2*np.pi*k*doy/365.25)
        df[f"cos_{k}"] = np.cos(2*np.pi*k*doy/365.25)

    # Cherry Blossom window (DC)
    def cherry_flag(d):
        start = pd.Timestamp(year=d.year, month=3, day=15)
        end   = pd.Timestamp(year=d.year, month=4, day=20)
        return int(start <= d <= end)
    df["is_cherry_blossom"] = df[date_col].apply(cherry_flag).astype("int8")

    # agency shares if present
    agency_cols = ["DDOT","DPW","MPD-1D","MPD-3D","OTHER","USCP"]
    existing_agencies = [c for c in agency_cols if c in df.columns]
    if existing_agencies:
        tot = df[existing_agencies].sum(axis=1).replace(0, np.nan)
        for c in existing_agencies:
            df[f"{c}_share"] = (df[c] / tot).fillna(0)

    # one-hot DOW (force numeric)
    dummies = pd.get_dummies(df["day_of_week"], prefix="dow", drop_first=True).astype("int8")
    df = pd.concat([df, dummies], axis=1)
    return df

def add_spatial_if_present(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "gridID" in df.columns:
        df["gridID_mean_target"] = df.groupby("gridID")[TARGET].transform("mean")
    if {"lat","lon"}.issubset(df.columns):
        lat0, lon0 = df["lat"].median(), df["lon"].median()
        df["dist_center"] = np.hypot(df["lat"]-lat0, df["lon"]-lon0)
    return df

def add_event_proxies(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["is_eom"] = (df[DATE_COL].dt.is_month_end).astype("int8")
    return df

def make_supervised(df: pd.DataFrame, horizons, target=TARGET, date_col=DATE_COL) -> pd.DataFrame:
    df = df.copy()
    for h in horizons:
        df[f"y_t_plus_{h}"] = df[target].shift(-h)
    return df.dropna().reset_index(drop=True)

def chrono_split(df: pd.DataFrame, date_col, train_end, val_end):
    train = df[df[date_col] <= train_end]
    val   = df[(df[date_col] > train_end) & (df[date_col] <= val_end)]
    test  = df[df[date_col] > val_end]
    return train, val, test

def evaluate(y_true, y_pred):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred)/np.maximum(y_true, 1))) * 100
    return {"MAE (tickets)": mae, "RMSE (tickets)": rmse, "MAPE (%)": mape}

def plot_pred_vs_actual(dates, y_true, y_pred, title, save_path):
    plt.figure(figsize=(11,4))
    plt.plot(dates, y_true, label="Actual", linewidth=2)
    plt.plot(dates, y_pred, label="Predicted", linestyle='--', linewidth=2)
    plt.title(title); plt.xlabel("Date"); plt.ylabel("Violations (tickets)")
    plt.xticks(rotation=45); plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(save_path, dpi=200); plt.close()

def plot_residual_hist(residuals, save_path, bins=20, label=None):
    plt.figure()
    plt.hist(residuals, bins=bins, alpha=0.7, label=label)
    if label: plt.legend()
    plt.xlabel("Residual (tickets)"); plt.ylabel("Count"); plt.title("Residual Distribution")
    plt.tight_layout(); plt.savefig(save_path, dpi=200); plt.close()

# ---------------------------------
# 4) Load + Feature Engineering
# ---------------------------------
daily_df   = load_and_patch_daily(DAILY_FILE)
feature_df = add_temporal_features(daily_df)
feature_df = add_spatial_if_present(feature_df)
feature_df = add_event_proxies(feature_df)
feature_df = feature_df.dropna().reset_index(drop=True)

# supervised dataset
supervised = make_supervised(feature_df, HORIZONS, target=TARGET, date_col=DATE_COL)

# ---------------------------------
# 5) Chronological Split
# ---------------------------------
train_df, val_df, test_df = chrono_split(supervised, DATE_COL, TRAIN_END, VAL_END)

# Build X (exclude raw target, future targets, raw DOW text) — NUMERIC ONLY
all_targets = [f"y_t_plus_{h}" for h in HORIZONS]
drop_cols   = [TARGET, DATE_COL, "day_of_week"] + all_targets
X_cols      = [c for c in supervised.columns
               if c not in drop_cols and pd.api.types.is_numeric_dtype(supervised[c])]

# Pack splits
data_by_h = {}
for h in HORIZONS:
    y_col = f"y_t_plus_{h}"
    data_by_h[h] = {
        "X_train": train_df[X_cols], "y_train": train_df[y_col],
        "X_val":   val_df[X_cols],   "y_val":   val_df[y_col],
        "X_test":  test_df[X_cols],  "y_test":  test_df[y_col],
        "dates_test": test_df[DATE_COL].values
    }

# ---------------------------------
# 6) Baselines (+GBDT) & SARIMAX
# ---------------------------------
model_zoo = {
    "LinearRegression": Pipeline([("scaler", StandardScaler()), ("lr", LinearRegression())]),
    "LassoCV": Pipeline([("scaler", StandardScaler()), ("lasso", LassoCV(cv=5, random_state=42, max_iter=10000))]),
    "RandomForest": RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1, min_samples_split=5),
    "GBDT": GradientBoostingRegressor(random_state=42)
}

metrics_rows, pred_store = [], {}

for h, data in data_by_h.items():
    pred_store[h] = {}
    X_train, y_train = data["X_train"], data["y_train"]
    X_test,  y_test  = data["X_test"],  data["y_test"]
    dates_test       = data["dates_test"]

    # supervised baselines
    for name, model in model_zoo.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        m = evaluate(y_test, y_pred)
        m.update({"Model": name, "Horizon": h})
        metrics_rows.append(m)
        pred_store[h][name] = y_pred

    # SARIMAX rolling forecast (weekly seasonality)
    ts_trainval = supervised.set_index(DATE_COL)[TARGET].asfreq('D').loc[:VAL_END]
    history = ts_trainval.copy()
    sar_preds = []
    for dt in dates_test:
        sar_model = SARIMAX(history,
                            order=(1,1,1),
                            seasonal_order=(1,1,1,7),
                            enforce_stationarity=False,
                            enforce_invertibility=False).fit(disp=False)
        forecast_val = sar_model.forecast(steps=h).iloc[-1]
        sar_preds.append(forecast_val)
        true_val = supervised.set_index(DATE_COL).loc[dt, TARGET]
        history  = pd.concat([history, pd.Series(true_val, index=[dt])]).asfreq('D')

    m = evaluate(y_test.values, np.array(sar_preds))
    m.update({"Model": "SARIMAX(1,1,1)(1,1,1,7)", "Horizon": h})
    metrics_rows.append(m)
    pred_store[h]["SARIMAX"] = np.array(sar_preds)

# Baseline metrics table: save + show + HTML
metrics_df = pd.DataFrame(metrics_rows).set_index(["Horizon", "Model"]).sort_values(["Horizon","MAE (tickets)"])
metrics_df.to_csv(OUTPUT_PATH / "baseline_metrics_multi_horizon.csv")
show_table(metrics_df, "Baseline metrics (Linear/Lasso/RF/GBDT/SARIMAX)")
save_html(metrics_df, OUTPUT_PATH / "baseline_metrics_multi_horizon.html")

# ---------------------------------
# 7) RF Feature Importance + save engineered features
# ---------------------------------
rf = model_zoo["RandomForest"]
rf.fit(data_by_h[1]["X_train"], data_by_h[1]["y_train"])
fi = pd.DataFrame({"feature": X_cols, "importance": rf.feature_importances_}).sort_values("importance", ascending=False)
fi.to_csv(OUTPUT_PATH / "rf_feature_importance.csv", index=False)
plt.figure(figsize=(8,6))
plt.barh(fi.head(15)["feature"][::-1], fi.head(15)["importance"][::-1])
plt.xlabel("Importance"); plt.title("RandomForest Feature Importance (Top 15)")
plt.tight_layout(); plt.savefig(OUTPUT_PATH / "rf_feature_importance.png", dpi=200); plt.close()

feature_df.to_csv(OUTPUT_PATH / "features_daily.csv", index=False)

# ============================================================
# 8) TabNet (per-horizon)  -- uses auto device via device_name
# ============================================================
TABNET_PARAMS = dict(
    n_d=32, n_a=32, n_steps=5, gamma=1.5,
    n_independent=2, n_shared=2,
    lambda_sparse=1e-4,
    optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-3),
    mask_type='entmax', seed=42, verbose=0,
    device_name=DEVICE_NAME
)
MAX_EPOCHS, PATIENCE = 200, 20
BATCH_SIZE, VIRTUAL_BS = 4096, 256

def to_tabnet_arrays(Xdf, yser):
    X = Xdf.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(np.float32).values
    y = pd.Series(yser).replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(np.float32).values.reshape(-1,1)
    return X, y

def extract_tabnet_curves(model):
    train_loss, val_curve = [], []
    hist = getattr(model, "history", None)
    if isinstance(hist, dict):
        train_loss = list(hist.get("loss", hist.get("train_loss", [])))
        for k in ["val_0","valid","val","loss_valid","valid_loss"]:
            if k in hist:
                v = hist[k]
                if isinstance(v, dict) and len(v):
                    if "mae" in v: val_curve = list(v["mae"])
                    else: val_curve = list(next(iter(v.values())))
                elif isinstance(v, list):
                    val_curve = list(v)
                break
    return train_loss, val_curve

tabnet_models, tabnet_preds, tabnet_rows = {}, {}, []

for h, data in data_by_h.items():
    Xtr, ytr = to_tabnet_arrays(data["X_train"], data["y_train"])
    Xva, yva = to_tabnet_arrays(data["X_val"],   data["y_val"])
    Xte, yte = to_tabnet_arrays(data["X_test"],  data["y_test"])

    m = TabNetRegressor(**TABNET_PARAMS)
    m.fit(
        Xtr, ytr,
        eval_set=[(Xva, yva)], eval_name=["valid"], eval_metric=["mae"],
        max_epochs=MAX_EPOCHS, patience=PATIENCE,
        batch_size=BATCH_SIZE, virtual_batch_size=VIRTUAL_BS,
        num_workers=0, drop_last=False
    )

    # save model .zip (optional)
    try:
        m.save_model(str(OUTPUT_PATH / f"tabnet_h{h}"))
        print(f"Saved TabNet model for t+{h} at", OUTPUT_PATH / f"tabnet_h{h}.zip")
    except Exception:
        pass

    yhat = m.predict(Xte).ravel()
    tabnet_models[h] = m; tabnet_preds[h] = yhat
    r = evaluate(yte.ravel(), yhat); r.update({"Model":"TabNet","Horizon":h})
    tabnet_rows.append(r)

    # save training curves CSV
    tr, va = extract_tabnet_curves(m)
    pd.DataFrame({"epoch": np.arange(len(tr)), "train_loss": tr,
                  "val_curve": va[:len(tr)] if len(va)>=len(tr) else va + [np.nan]*(len(tr)-len(va))})\
      .to_csv(OUTPUT_PATH / f"tabnet_training_h{h}.csv", index=False)

# Merge TabNet with baselines & show
tabnet_df  = pd.DataFrame(tabnet_rows).set_index(["Horizon","Model"])
metrics_df = pd.concat([metrics_df, tabnet_df]).sort_values(["Horizon","MAE (tickets)"])
metrics_df.to_csv(OUTPUT_PATH / "metrics_with_tabnet.csv")
show_table(metrics_df, "Metrics with TabNet")
save_html(metrics_df, OUTPUT_PATH / "metrics_with_tabnet.html")

# Save TabNet per-horizon predictions & totals, show totals
totals=[]
for h, data in data_by_h.items():
    dates = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    out = pd.DataFrame({"date": dates, "actual": data["y_test"].values, "tabnet": tabnet_preds[h]})
    out.to_csv(OUTPUT_PATH / f"preds_tabnet_t+{h}.csv", index=False)
    frame = out.dropna()
    totals.append({"Horizon": h, "Total Actual (tickets)": frame["actual"].sum(),
                   "Total Predicted (tickets)": frame["tabnet"].sum()})

totals_df = pd.DataFrame(totals).set_index("Horizon")
totals_df.to_csv(OUTPUT_PATH / "tabnet_total_tickets_test.csv")
show_table(totals_df, "Totals (test window): actual vs TabNet predicted tickets")
save_html(totals_df, OUTPUT_PATH / "tabnet_total_tickets_test.html")

# Per-horizon plots + combined overlay/histograms
for h, data in data_by_h.items():
    target_dates = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    y_true = data["y_test"].values.astype(np.float32)
    y_pred = tabnet_preds[h].astype(np.float32)

    plot_pred_vs_actual(target_dates, y_true, y_pred,
                        f"TabNet: Pred vs Actual – t+{h}", OUTPUT_PATH / f"tabnet_pred_vs_actual_h{h}.png")

    resid = y_true - y_pred
    plot_residual_hist(resid, OUTPUT_PATH / f"tabnet_residual_hist_h{h}.png")

    plt.figure(figsize=(5,5))
    plt.scatter(y_true, y_pred, s=8, alpha=0.6)
    ax = plt.gca()
    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]
    plt.plot(lims, lims, '--')
    plt.title(f"Calibration – t+{h}"); plt.xlabel("Actual (tickets)"); plt.ylabel("Predicted (tickets)")
    plt.tight_layout(); plt.savefig(OUTPUT_PATH / f"tabnet_calibration_h{h}.png", dpi=200); plt.close()

# ALL horizons overlay (last 90 days)
overlay_parts = []
for h, data in data_by_h.items():
    target_dates = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    overlay_parts.append(pd.DataFrame({"date": target_dates, f"pred_t+{h}": tabnet_preds[h]}).set_index("date"))
overlay = pd.concat(overlay_parts, axis=1).sort_index()
actual = supervised.set_index(DATE_COL)[TARGET].reindex(overlay.index)
overlay_full = pd.concat([actual.rename("actual"), overlay], axis=1).dropna()

N=90; sub=overlay_full.tail(N)
plt.figure(figsize=(12,5))
plt.plot(sub.index, sub["actual"], label="Actual", linewidth=2)
for h in sorted(tabnet_preds):
    plt.plot(sub.index, sub[f"pred_t+{h}"], "--", label=f"TabNet t+{h}")
plt.title("Actual vs. TabNet forecasts (t+1, t+3, t+7) — single overlay")
plt.xlabel("Date"); plt.ylabel("Violations (tickets)")
plt.xticks(rotation=45); plt.grid(True); plt.legend(); plt.tight_layout()
plt.savefig(OUTPUT_PATH / "tabnet_overlay_all_horizons.png", dpi=200); plt.close()

# Combined residual histogram (all horizons one figure)
plt.figure(figsize=(8,5))
bins = 20
for h, data in data_by_h.items():
    y_true = data["y_test"].values.astype(np.float32)
    y_pred = tabnet_preds[h].astype(np.float32)
    plt.hist(y_true - y_pred, bins=bins, alpha=0.5, label=f"t+{h}")
plt.title("Residual histograms (TabNet) – all horizons"); plt.xlabel("Residual (tickets)"); plt.ylabel("Count")
plt.legend(); plt.tight_layout(); plt.savefig(OUTPUT_PATH / "tabnet_residual_hist_all_in_one.png", dpi=200); plt.close()

# 3D residual histogram (TabNet)
fig = plt.figure(figsize=(10,6)); ax = fig.add_subplot(111, projection='3d')
all_resid = []
for h, data in data_by_h.items():
    y_true = data["y_test"].values.astype(np.float32)
    y_pred = tabnet_preds[h].astype(np.float32)
    all_resid.append(y_true - y_pred)
res_min = min(map(np.min, all_resid)); res_max = max(map(np.max, all_resid))
bin_edges = np.linspace(res_min, res_max, 18)
xpos_list, ypos_list, zpos_list, dx_list, dy_list, dz_list = [], [], [], [], [], []
for yi, h in enumerate(sorted(tabnet_preds)):
    counts, edges = np.histogram(all_resid[yi], bins=bin_edges)
    centers = 0.5*(edges[1:]+edges[:-1])
    xpos_list.extend(centers); ypos_list.extend([yi]*len(centers)); zpos_list.extend([0]*len(centers))
    dx_list.extend(np.diff(edges)); dy_list.extend([0.6]*len(centers)); dz_list.extend(counts)
ax.bar3d(xpos_list, ypos_list, zpos_list, dx_list, dy_list, dz_list, shade=True)
ax.set_xlabel("Residual"); ax.set_ylabel("Horizon idx (0=t+1,1=t+3,2=t+7)"); ax.set_zlabel("Count")
ax.set_title("3D residual histogram — TabNet"); plt.tight_layout()
plt.savefig(OUTPUT_PATH / "tabnet_residual_hist_3D.png", dpi=200); plt.close()

# ============================================================
# 9) Multi-task TabNet (predict t+1,t+3,t+7 jointly)
# ============================================================
def build_multitask(df, horizons, X_cols, date_col):
    X = df[X_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float32).values
    Y = np.column_stack([df[f"y_t_plus_{h}"].values.astype(np.float32) for h in horizons])
    dates = df[date_col].values
    return X, Y, dates

Xtr_mt, Ytr_mt, _ = build_multitask(train_df, HORIZONS, X_cols, DATE_COL)
Xva_mt, Yva_mt, _ = build_multitask(val_df,   HORIZONS, X_cols, DATE_COL)
Xte_mt, Yte_mt, _ = build_multitask(test_df,  HORIZONS, X_cols, DATE_COL)

mt_params = dict(
    n_d=32, n_a=32, n_steps=5, gamma=1.5, n_independent=2, n_shared=2,
    lambda_sparse=1e-4, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-3),
    mask_type='entmax', seed=42, verbose=0, device_name=DEVICE_NAME
)
mt = TabNetRegressor(**mt_params)
mt.fit(
    Xtr_mt, Ytr_mt, eval_set=[(Xva_mt, Yva_mt)], eval_name=["valid"],
    eval_metric=["mae"], max_epochs=200, patience=20, batch_size=4096, virtual_batch_size=256, num_workers=0
)
Yhat_mt = mt.predict(Xte_mt)

mt_rows = []
for i, h in enumerate(HORIZONS):
    y_true = Yte_mt[:, i]; y_pred = Yhat_mt[:, i]
    r = evaluate(y_true, y_pred); r.update({"Model":"TabNet-Multitask","Horizon":h})
    mt_rows.append(r)
mt_df = pd.DataFrame(mt_rows).set_index(["Horizon","Model"])
metrics_df = pd.concat([metrics_df, mt_df]).sort_values(["Horizon","MAE (tickets)"])
metrics_df.to_csv(OUTPUT_PATH / "metrics_with_tabnet_and_multitask.csv")
show_table(metrics_df, "Metrics with TabNet + Multitask")
save_html(metrics_df, OUTPUT_PATH / "metrics_with_tabnet_and_multitask.html")

# ============================================================
# 10) Conformal prediction intervals (TabNet per horizon)
# ============================================================
def conformal_band(y_val, y_val_pred, y_test_pred, q=0.9):
    alpha = np.quantile(np.abs(y_val - y_val_pred), q)
    lo = y_test_pred - alpha; hi = y_test_pred + alpha
    return lo, hi, alpha

coverage_rows = []
for h in HORIZONS:
    Xv = data_by_h[h]["X_val"].replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float32).values
    yv = data_by_h[h]["y_val"].values.astype(np.float32)
    pv = tabnet_models[h].predict(Xv).ravel().astype(np.float32)

    dates = pd.to_datetime(data_by_h[h]["dates_test"]) + pd.to_timedelta(h, "D")
    pt   = np.array(tabnet_preds[h], dtype=np.float32)
    lo, hi, alpha = conformal_band(yv, pv, pt, q=0.9)

    plt.figure(figsize=(12,4))
    plt.plot(dates, data_by_h[h]["y_test"].values, label="Actual", lw=2)
    plt.plot(dates, pt, "--", label="TabNet mean", lw=2)
    plt.fill_between(dates, lo, hi, alpha=0.18, label="Conformal 90% band")
    plt.title(f"TabNet t+{h} with conformal intervals (|resid| q=0.90, α≈{alpha:.1f})")
    plt.xlabel("Target date"); plt.ylabel("Violations (tickets)"); plt.grid(True); plt.legend()
    plt.tight_layout(); plt.savefig(OUTPUT_PATH/f"tabnet_conformal_h{h}.png", dpi=200); plt.close()

    yt = data_by_h[h]["y_test"].values.astype(np.float32)
    cov = np.mean((yt >= lo) & (yt <= hi))
    coverage_rows.append({"Horizon": h, "Nominal": 0.90, "Empirical": float(cov), "Alpha(abs resid q)": float(alpha)})

cov_df = pd.DataFrame(coverage_rows)
cov_df.to_csv(OUTPUT_PATH / "conformal_coverage_tabnet.csv", index=False)
show_table(cov_df, "Conformal coverage (TabNet, per horizon)")
save_html(cov_df, OUTPUT_PATH / "conformal_coverage_tabnet.html")

# ============================================================
# 11) Rolling-origin backtesting (quick K=3)
# ============================================================
K = 3
fold_rows = []
dates_all = supervised[DATE_COL]
cutoffs = pd.date_range(dates_all.min()+pd.Timedelta(days=365),
                        dates_all.max()-pd.Timedelta(days=90), periods=K+1)[1:]

for k, cutoff in enumerate(cutoffs, 1):
    tr = supervised[supervised[DATE_COL] <= cutoff]
    te = supervised[supervised[DATE_COL] >  cutoff]
    Xtr = tr[X_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float32).values
    for h in HORIZONS:
        ytr = tr[f"y_t_plus_{h}"].values.astype(np.float32).reshape(-1,1)
        Xte = te[X_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float32).values
        yte = te[f"y_t_plus_{h}"].values.astype(np.float32)
        model = TabNetRegressor(**TABNET_PARAMS)
        model.fit(Xtr, ytr, max_epochs=100, patience=15, batch_size=4096, virtual_batch_size=256, num_workers=0)
        yhat = model.predict(Xte).ravel()
        mae  = mean_absolute_error(yte, yhat)
        rmse = np.sqrt(mean_squared_error(yte, yhat))
        fold_rows.append({"Fold": k, "Horizon": h, "MAE": mae, "RMSE": rmse})

ro_df = pd.DataFrame(fold_rows)
ro_df.to_csv(OUTPUT_PATH/"tabnet_rolling_origin_cv.csv", index=False)
show_table(ro_df, "Rolling-origin CV (TabNet)")
save_html(ro_df, OUTPUT_PATH / "tabnet_rolling_origin_cv.html")

# ============================================================
# 12) Best baseline vs TabNet overlay (per horizon) + save baseline preds
# ============================================================
for h, data in data_by_h.items():
    sub = metrics_df.loc[h].reset_index()
    sub_bl = sub[~sub["Model"].str.contains("TabNet|LSTM|Transformer", case=False)]
    best_name = sub_bl.sort_values("MAE (tickets)").iloc[0]["Model"]
    baseline_preds = pred_store[h][best_name]
    dates = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    plt.figure(figsize=(11,4))
    plt.plot(dates, data["y_test"].values, label="Actual", lw=2)
    plt.plot(dates, baseline_preds, ":", label=f"{best_name}", lw=2)
    plt.plot(dates, tabnet_preds[h], "--", label="TabNet", lw=2)
    plt.title(f"Best baseline vs TabNet — t+{h}")
    plt.xlabel("Target date"); plt.ylabel("Violations (tickets)")
    plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(OUTPUT_PATH/f"tabnet_vs_bestbaseline_h{h}.png", dpi=200); plt.close()

    pd.DataFrame({"date": dates, "actual": data["y_test"].values, f"{best_name}": baseline_preds})\
      .to_csv(OUTPUT_PATH / f"preds_best_baseline_t+{h}.csv", index=False)

# ============================================================
# 13) TabTransformer (Keras) on SAME SPLITS (unique input names)
# ============================================================
cat_cols = []
if "day_of_week" in supervised.columns: cat_cols.append("day_of_week")
for c in ["is_holiday","is_weekend"]:
    if c in supervised.columns: cat_cols.append(c)

# Avoid using DOW one-hot columns; use the raw numeric features for TF model
num_cols_tf = [c for c in X_cols if not c.startswith("dow_")]

def build_tabtransformer(cat_cols, num_cols):
    inputs, cat_encoded = [], []
    for col in cat_cols:
        _inp_name = f"cat_{col}"  # ensure UNIQUE names
        dtype = "string" if supervised[col].dtype == object else "int64"
        inp = keras.Input(shape=(1,), name=_inp_name, dtype=dtype)
        if dtype == "string":
            vocab = sorted(supervised[col].astype(str).unique().tolist())
            lookup = layers.StringLookup(vocabulary=vocab, output_mode="int")
        else:
            vocab = sorted(supervised[col].astype(int).unique().tolist())
            lookup = layers.IntegerLookup(vocabulary=vocab, output_mode="int")
        emb = layers.Embedding(input_dim=len(vocab)+1, output_dim=16)
        x = emb(lookup(inp))
        inputs.append(inp); cat_encoded.append(x)

    if cat_encoded:
        cat_stack = layers.Concatenate(axis=1)(cat_encoded)
        for _ in range(2):
            attn = layers.MultiHeadAttention(num_heads=2, key_dim=16)(cat_stack, cat_stack)
            x = layers.Add()([attn, cat_stack]); x = layers.LayerNormalization(epsilon=1e-6)(x)
            ffn = layers.Dense(64, activation="relu")(x); ffn = layers.Dense(16)(ffn)
            cat_stack = layers.Add()([cat_stack, ffn]); cat_stack = layers.LayerNormalization(epsilon=1e-6)(cat_stack)
        cat_flat = layers.Flatten()(cat_stack)
    else:
        cat_flat = None

    num_inputs, num_encoded = [], []
    for col in num_cols:
        _inp_name = f"num_{col}"  # ensure UNIQUE names
        inp = keras.Input(shape=(1,), name=_inp_name, dtype="float32")
        norm = layers.Normalization()
        norm.adapt(train_df[col].values.astype(np.float32).reshape(-1,1))  # adapt on TRAIN only
        x = norm(inp); num_inputs.append(inp); num_encoded.append(x)
    num_concat = layers.Concatenate()(num_encoded) if num_encoded else None

    if cat_flat is not None and num_concat is not None:
        combined = layers.Concatenate()([cat_flat, num_concat])
    elif cat_flat is not None:
        combined = cat_flat
    else:
        combined = num_concat

    x = layers.Dense(128, activation="relu")(combined)
    x = layers.Dense(64, activation="relu")(x)
    out = layers.Dense(1)(x)
    model = keras.Model(inputs=inputs+num_inputs, outputs=out)
    model.compile(optimizer="adam", loss="mse", metrics=["mae"])
    return model

def df_to_ttx(df):
    feed = []
    for c in cat_cols:
        if df[c].dtype != object:
            feed.append(df[c].astype("int64").values)
        else:
            feed.append(df[c].astype(str).values)
    for c in num_cols_tf:
        feed.append(df[c].astype(np.float32).values)
    return feed

tt_rows, tt_preds = [], {}
for h in HORIZONS:
    y_col = f"y_t_plus_{h}"
    ttm = build_tabtransformer(cat_cols, num_cols_tf)
    cb  = [keras.callbacks.EarlyStopping(monitor="val_mae", patience=10, restore_best_weights=True)]
    ttm.fit(df_to_ttx(train_df), train_df[y_col].values,
            validation_data=(df_to_ttx(val_df), val_df[y_col].values),
            epochs=100, batch_size=64, verbose=0, callbacks=cb)
    yhat = ttm.predict(df_to_ttx(test_df), verbose=0).ravel()
    r = evaluate(test_df[y_col].values, yhat); r.update({"Model": "TabTransformer", "Horizon": h})
    tt_rows.append(r)

    dates = pd.to_datetime(test_df[DATE_COL].values) + pd.to_timedelta(h, "D")
    tt_preds[h] = {"dates": dates, "y_pred": yhat}
    pd.DataFrame({"date": dates, "actual": test_df[y_col].values, "tabtransformer": yhat})\
      .to_csv(OUTPUT_PATH/f"preds_tabtransformer_t+{h}.csv", index=False)

    plt.figure(figsize=(11,4))
    plt.plot(dates, test_df[y_col].values, label="Actual", lw=2)
    plt.plot(dates, yhat, "--", label="TabTransformer", lw=2)
    plt.title(f"TabTransformer — t+{h}"); plt.xlabel("Target date"); plt.ylabel("Violations (tickets)")
    plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(OUTPUT_PATH/f"tabtransformer_h{h}.png", dpi=200); plt.close()

tt_df = pd.DataFrame(tt_rows).set_index(["Horizon","Model"])
metrics_df = pd.concat([metrics_df, tt_df]).sort_values(["Horizon","MAE (tickets)"])
metrics_df.to_csv(OUTPUT_PATH / "metrics_with_tabnet_transformer.csv")
show_table(metrics_df, "Metrics with TabNet + Multitask + TabTransformer")
save_html(metrics_df, OUTPUT_PATH / "metrics_with_tabnet_transformer.html")

# ============================================================
# 14) LSTM multi-output (predict t+1,t+3,t+7 together)
# ============================================================
LOOKBACK = 30
HOUT = len(HORIZONS)
MAX_H = max(HORIZONS)

def build_multioutput_sequences(series, dates, horizons, lookback=LOOKBACK):
    X, Y, T = [], [], []
    for i in range(lookback, len(series)-MAX_H+1):
        X.append(series[i-lookback:i])
        Y.append([series[i+h-1] for h in horizons])
        T.append(dates[i+MAX_H-1])
    X = np.array(X, dtype=np.float32).reshape(-1, lookback, 1)
    Y = np.array(Y, dtype=np.float32)
    T = np.array(T)
    return X, Y, T

def split_multi_by_dates(X, Y, T, train_end, val_end):
    mtr = T <= np.datetime64(train_end)
    mva = (T > np.datetime64(train_end)) & (T <= np.datetime64(val_end))
    mte = T > np.datetime64(val_end)
    return (X[mtr], Y[mtr]), (X[mva], Y[mva]), (X[mte], Y[mte]), T[mte]

def build_lstm_mt(lookback=LOOKBACK, n_out=HOUT):
    inp = keras.Input(shape=(lookback,1), name="lstm_input")
    x = layers.LSTM(64)(inp)
    x = layers.Dense(64, activation="relu")(x)
    out = layers.Dense(n_out, name="lstm_out")(x)
    m = keras.Model(inp, out)
    m.compile(optimizer="adam", loss="mse", metrics=["mae"])
    return m

series = supervised[TARGET].values.astype(np.float32)
dates  = supervised[DATE_COL].values

X_all, Y_all, T_all = build_multioutput_sequences(series, dates, HORIZONS, LOOKBACK)
(trainX, trainY), (valX, valY), (testX, testY), testT = split_multi_by_dates(X_all, Y_all, T_all, TRAIN_END, VAL_END)

lstm_mt = build_lstm_mt(LOOKBACK, HOUT)
cb = [keras.callbacks.EarlyStopping(monitor="val_mae", patience=10, restore_best_weights=True)]
lstm_mt.fit(trainX, trainY, validation_data=(valX,valY), epochs=100, batch_size=64, verbose=0, callbacks=cb)

Yhat_lstm_mt = lstm_mt.predict(testX, verbose=0)  # [n, len(HORIZONS)]

lstm_mt_preds, lstm_mt_rows = {}, []
for j, h in enumerate(HORIZONS):
    dates_h = pd.to_datetime(testT) - pd.to_timedelta(MAX_H - h, "D")
    y_true_h = testY[:, j]
    y_pred_h = Yhat_lstm_mt[:, j]
    lstm_mt_preds[h] = {"dates": dates_h, "y_pred": y_pred_h}
    r = evaluate(y_true_h, y_pred_h); r.update({"Model":"LSTM-Multitask","Horizon":h})
    lstm_mt_rows.append(r)

    pd.DataFrame({"date": dates_h, "actual": y_true_h, "lstm_mt": y_pred_h})\
      .to_csv(OUTPUT_PATH / f"preds_lstm_mt_t+{h}.csv", index=False)

    plt.figure(figsize=(11,4))
    plt.plot(dates_h, y_true_h, label="Actual", lw=2)
    plt.plot(dates_h, y_pred_h, "--", label="LSTM-MT", lw=2)
    plt.title(f"LSTM Multi-Output — t+{h}")
    plt.xlabel("Target date"); plt.ylabel("Violations (tickets)"); plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(OUTPUT_PATH/f"lstm_multitask_h{h}.png", dpi=200); plt.close()

lstm_mt_df = pd.DataFrame(lstm_mt_rows).set_index(["Horizon","Model"])
metrics_df = pd.concat([metrics_df, lstm_mt_df]).sort_values(["Horizon","MAE (tickets)"])

# --------- Add MASE and save master leaderboard (CSV + HTML + show) ---------
denom = np.mean(np.abs(np.diff(supervised.loc[supervised[DATE_COL] <= TRAIN_END, TARGET].values)))
metrics_all = metrics_df.reset_index()
if "MASE" not in metrics_all.columns:
    metrics_all["MASE"] = metrics_all["MAE (tickets)"] / max(denom, 1e-6)
metrics_all.to_csv(OUTPUT_PATH / "metrics_everything.csv", index=False)
show_table(metrics_all, "Leaderboard — ALL models (baseline + SARIMAX + TabNet + MT + LSTM + TabTransformer)")
save_html(metrics_all, OUTPUT_PATH / "metrics_everything.html")

# ============================================================
# 15) Best tree learning curves (train vs val) — save + show CSVs
# ============================================================
def _best_tree_name_for_h(h, metrics_df_):
    sub = metrics_df_.loc[h].reset_index()
    sub = sub[sub["Model"].isin(["RandomForest","GBDT"])]
    return sub.sort_values("MAE (tickets)").iloc[0]["Model"]

def _make_tree_est(name):
    return RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1, min_samples_split=5) \
           if name == "RandomForest" else GradientBoostingRegressor(random_state=42)

for h, data in data_by_h.items():
    best_name = _best_tree_name_for_h(h, metrics_df)
    base_est  = _make_tree_est(best_name)
    Xtr_full, ytr_full = data["X_train"], data["y_train"]
    Xva,      yva      = data["X_val"],   data["y_val"]
    sizes, rows = np.linspace(0.2, 1.0, 10), []
    for s in sizes:
        n   = max(20, int(len(Xtr_full) * s))
        est = clone(base_est)
        Xtr = Xtr_full.iloc[:n]; ytr = ytr_full.iloc[:n]
        est.fit(Xtr, ytr)
        tr_rmse = np.sqrt(mean_squared_error(ytr, est.predict(Xtr)))
        va_rmse = np.sqrt(mean_squared_error(yva, est.predict(Xva)))
        rows.append({"frac_train": s, "train_RMSE": tr_rmse, "val_RMSE": va_rmse})
    curve_df = pd.DataFrame(rows)
    curve_df.to_csv(OUTPUT_PATH / f"best_tree_learning_curve_h{h}.csv", index=False)
    show_table(curve_df, f"{best_name} learning curve — t+{h}")
    save_html(curve_df, OUTPUT_PATH / f"best_tree_learning_curve_h{h}.html")
    plt.figure(figsize=(8,4))
    plt.plot(curve_df["frac_train"], curve_df["train_RMSE"], marker="o", label="Train RMSE")
    plt.plot(curve_df["frac_train"], curve_df["val_RMSE"], marker="o", label="Val RMSE")
    plt.title(f"{best_name}: training vs validation RMSE — t+{h}")
    plt.xlabel("Fraction of training data used (chronological)")
    plt.ylabel("RMSE (tickets)")
    plt.grid(True); plt.legend(); plt.tight_layout()
    plt.savefig(OUTPUT_PATH / f"best_tree_learning_curve_h{h}.png", dpi=200); plt.close()

# ============================================================
# 16) SOTA showdown (per horizon) — all key models overlay
# ============================================================
for h, data in data_by_h.items():
    dates_h = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    actual  = data["y_test"].values
    sub = metrics_df.loc[h].reset_index()
    sub_bl = sub[~sub["Model"].str.contains("TabNet|LSTM|Transformer", case=False)]
    best_name = sub_bl.sort_values("MAE (tickets)").iloc[0]["Model"]
    best_pred = pred_store[h][best_name]
    idx = HORIZONS.index(h)

    plt.figure(figsize=(12,5))
    plt.plot(dates_h, actual, label="Actual", lw=2)
    plt.plot(dates_h, best_pred, ":", lw=2, label=f"Best baseline: {best_name}")
    plt.plot(dates_h, tabnet_preds[h], "--", lw=2, label="TabNet")
    plt.plot(dates_h, Yhat_mt[:, idx], "-", lw=1.8, label="TabNet-MT")
    plt.plot(lstm_mt_preds[h]["dates"], lstm_mt_preds[h]["y_pred"], "-.", lw=1.8, label="LSTM-MT")
    if h in tt_preds:
        plt.plot(tt_preds[h]["dates"], tt_preds[h]["y_pred"], lw=1.5, label="TabTransformer")
    plt.title(f"SOTA showdown — t+{h}")
    plt.xlabel("Target date"); plt.ylabel("Violations (tickets)")
    plt.grid(True); plt.legend(ncol=2); plt.tight_layout()
    plt.savefig(OUTPUT_PATH / f"sota_showdown_h{h}.png", dpi=200); plt.close()

# ============================================================
# 16B) ALL-MODELS overlay (per horizon) + CSV (full + last 90d)
# ============================================================
def _build_all_model_overlay(h):
    """Return DataFrame indexed by target date with Actual and every model's prediction."""
    dates_common = pd.to_datetime(data_by_h[h]["dates_test"]) + pd.to_timedelta(h, "D")
    idx = HORIZONS.index(h)
    series_map = {}
    series_map["Actual"] = pd.Series(data_by_h[h]["y_test"].values, index=dates_common)
    for name in ["LinearRegression","LassoCV","RandomForest","GBDT","SARIMAX"]:
        if name in pred_store[h]:
            series_map[name] = pd.Series(pred_store[h][name], index=dates_common)
    if h in tabnet_preds:
        series_map["TabNet"] = pd.Series(tabnet_preds[h], index=dates_common)
    if 'Yhat_mt' in globals():
        series_map["TabNet-MT"] = pd.Series(Yhat_mt[:, idx], index=dates_common)
    if 'lstm_mt_preds' in globals() and h in lstm_mt_preds:
        s = pd.Series(lstm_mt_preds[h]["y_pred"], index=pd.to_datetime(lstm_mt_preds[h]["dates"]))
        series_map["LSTM-MT"] = s.reindex(dates_common)
    if 'tt_preds' in globals() and h in tt_preds:
        s = pd.Series(tt_preds[h]["y_pred"], index=pd.to_datetime(tt_preds[h]["dates"]))
        series_map["TabTransformer"] = s.reindex(dates_common)
    df_overlay = pd.DataFrame(series_map).sort_index()
    return df_overlay

def _plot_all_models_overlay(df_overlay, h, save_stub):
    plt.figure(figsize=(14,6))
    for col in df_overlay.columns:
        if col == "Actual":
            plt.plot(df_overlay.index, df_overlay[col], label=col, linewidth=2.5)
        else:
            plt.plot(df_overlay.index, df_overlay[col], label=col, linewidth=1.5, linestyle="--")
    plt.title(f"All-models overlay — t+{h}")
    plt.xlabel("Target date"); plt.ylabel("Violations (tickets)")
    plt.grid(True)
    plt.legend(ncol=2, bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0.)
    plt.tight_layout()
    plt.savefig(OUTPUT_PATH / f"{save_stub}_t+{h}.png", dpi=200)
    plt.close()

for h in HORIZONS:
    df_all = _build_all_model_overlay(h)
    df_all.to_csv(OUTPUT_PATH / f"all_model_preds_t+{h}.csv", index_label="date")
    _plot_all_models_overlay(df_all, h, save_stub="all_models_overlay_full")
    tail_n = min(90, len(df_all))
    if tail_n > 0:
        _plot_all_models_overlay(df_all.tail(tail_n), h, save_stub="all_models_overlay_last90")

for h in HORIZONS:
    try:
        show_table(pd.read_csv(OUTPUT_PATH / f"all_model_preds_t+{h}.csv").head(8),
                   f"Preview — all_model_preds_t+{h}.csv (first 8 rows)")
    except Exception:
        pass

# ============================================================
# 17) METRICS DASHBOARD quick plots from metrics_everything.csv
# ============================================================
dfm = pd.read_csv(OUTPUT_PATH / "metrics_everything.csv")
# bar chart per horizon
for h in HORIZONS:
    sub = dfm[dfm["Horizon"]==h].sort_values("MAE (tickets)")
    plt.figure(figsize=(10,5))
    plt.bar(sub["Model"], sub["MAE (tickets)"])
    plt.xticks(rotation=30, ha="right"); plt.ylabel("MAE (tickets)")
    plt.title(f"Leaderboard (MAE) — t+{h}")
    plt.tight_layout(); plt.savefig(OUTPUT_PATH / f"leaderboard_mae_t+{h}.png", dpi=200); plt.close()

# heatmap-like grid (matplotlib)
models = sorted(dfm["Model"].unique())
mat = np.zeros((len(HORIZONS), len(models)))
for i,h in enumerate(HORIZONS):
    sub = dfm[dfm["Horizon"]==h].set_index("Model")
    for j,m in enumerate(models):
        mat[i,j] = sub.loc[m, "MAE (tickets)"] if m in sub.index else np.nan
plt.figure(figsize=(max(8,len(models)*0.6), 4))
plt.imshow(mat, aspect="auto")
plt.xticks(range(len(models)), models, rotation=30, ha="right")
plt.yticks(range(len(HORIZONS)), [f"t+{h}" for h in HORIZONS])
plt.colorbar(label="MAE (tickets)")
plt.title("Leaderboard heatmap (MAE)")
plt.tight_layout(); plt.savefig(OUTPUT_PATH / "leaderboard_mae_heatmap.png", dpi=200); plt.close()

print("\n✔ All artifacts saved to:", OUTPUT_PATH)

# ============================================================
# 18) MEGA PLOT — All models × horizons in one figure (+ zoom)
#     Saves: big plot, last-90, legend-only, data CSV, color map
# ============================================================

from matplotlib.lines import Line2D
import colorsys

def _distinct_colors(n):
    # H S V grid: n evenly spaced hues; fixed S,V for vivid but readable
    cols = [colorsys.hsv_to_rgb(h, 0.65, 0.95) for h in np.linspace(0, 1, n, endpoint=False)]
    return [(r, g, b) for (r, g, b) in cols]

# 1) Gather ALL series (label, dates, values)
series = []

# Baselines (make sure order matches what you fit)
baseline_names = ["LinearRegression", "LassoCV", "RandomForest", "GBDT", "SARIMAX(1,1,1)(1,1,1,7)"]
for h, data in data_by_h.items():
    tgt_dates = pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D")
    for name in baseline_names:
        if h in pred_store and name in pred_store[h]:
            yhat = np.asarray(pred_store[h][name]).ravel()
            series.append((f"{name} t+{h}", h, tgt_dates, yhat))

# TabNet (per-horizon)
for h in HORIZONS:
    if h in tabnet_preds:
        tgt_dates = pd.to_datetime(data_by_h[h]["dates_test"]) + pd.to_timedelta(h, "D")
        series.append((f"TabNet t+{h}", h, tgt_dates, np.asarray(tabnet_preds[h]).ravel()))

# TabNet-Multitask
if "Yhat_mt" in globals():
    for j, h in enumerate(HORIZONS):
        dates_h = pd.to_datetime(test_df[DATE_COL].values) + pd.to_timedelta(h, "D")
        series.append((f"TabNet-MT t+{h}", h, dates_h, np.asarray(Yhat_mt[:, j]).ravel()))

# LSTM-MT
if "lstm_mt_preds" in globals():
    for h in HORIZONS:
        if h in lstm_mt_preds:
            series.append((f"LSTM-MT t+{h}", h, pd.to_datetime(lstm_mt_preds[h]["dates"]),
                           np.asarray(lstm_mt_preds[h]["y_pred"]).ravel()))

# TabTransformer
if "tt_preds" in globals():
    for h in HORIZONS:
        if h in tt_preds:
            series.append((f"TabTransformer t+{h}", h, pd.to_datetime(tt_preds[h]["dates"]),
                           np.asarray(tt_preds[h]["y_pred"]).ravel()))

# 2) Build union of dates across all series for an "Actual" reference line
all_dates = pd.Index([])
for _, _, dts, _ in series:
    all_dates = all_dates.union(pd.Index(dts))
all_dates = all_dates.sort_values()

actual_map = supervised.set_index(DATE_COL)[TARGET]
actual_on_union = actual_map.reindex(all_dates)

# 3) Assign unique colors to every prediction (actual stays black)
n_pred = len(series)
colors = _distinct_colors(n_pred)
label_to_color = {series[i][0]: colors[i] for i in range(n_pred)}

# 4) Save a long, tidy CSV for re-plotting later
rows = []
for label, h, dts, vals in series:
    for dt, v in zip(pd.to_datetime(dts), vals):
        rows.append({"date": dt, "model": label.split(" t+")[0], "horizon": h, "y_pred": float(v)})
mega_df = pd.DataFrame(rows).sort_values(["date", "model", "horizon"])
mega_df.to_csv(OUTPUT_PATH / "mega_all_models_all_horizons.csv", index=False)

# Also save color map for the legend
cmap_rows = [{"series_label": lbl, "color_rgb": str(tuple(np.round(np.array(col)*255).astype(int)))}
             for lbl, col in label_to_color.items()]
pd.DataFrame(cmap_rows).to_csv(OUTPUT_PATH / "mega_color_map.csv", index=False)

# 5) Plot — FULL RANGE
plt.figure(figsize=(18, 9))
# Actual
plt.plot(all_dates, actual_on_union.values, color="black", linewidth=2.2, label="Actual")

# Predictions (tons of lines)
for label, h, dts, vals in series:
    plt.plot(dts, vals, linewidth=1.4, alpha=0.9, label=label, color=label_to_color[label])

plt.title("ALL MODELS × HORIZONS — One Mega Comparison Plot")
plt.xlabel("Date"); plt.ylabel("Violations (tickets)")
plt.grid(True, alpha=0.25)
# Put a light legend on a separate figure (to avoid crowding here)
plt.tight_layout()
plt.savefig(OUTPUT_PATH / "mega_all_models_all_horizons.png", dpi=200)
plt.close()

# 6) Plot — LAST 90 DAYS
if len(all_dates) > 0:
    last90_start = all_dates.max() - pd.Timedelta(days=90)
    mask90 = all_dates >= last90_start
    dates90 = all_dates[mask90]
    actual90 = actual_on_union.loc[dates90]

    plt.figure(figsize=(18, 9))
    plt.plot(dates90, actual90.values, color="black", linewidth=2.2, label="Actual (last 90d)")
    for label, h, dts, vals in series:
        m = (dts >= last90_start)
        if np.any(m):
            plt.plot(pd.to_datetime(dts)[m], np.asarray(vals)[m],
                     linewidth=1.6, alpha=0.95, label=label, color=label_to_color[label])
    plt.title("ALL MODELS × HORIZONS — Last 90 Days")
    plt.xlabel("Date"); plt.ylabel("Violations (tickets)")
    plt.grid(True, alpha=0.25)
    plt.tight_layout()
    plt.savefig(OUTPUT_PATH / "mega_all_models_all_horizons_last90.png", dpi=200)
    plt.close()

# 7) Legend-only image (so you can drop it beside the figure in slides)
legend_items = [Line2D([0], [0], color="black", lw=2.2, label="Actual")]
legend_items += [Line2D([0], [0], color=label_to_color[lbl], lw=2.0, label=lbl) for lbl, _, _, _ in series]

fig = plt.figure(figsize=(12, max(6, 0.25 * (len(legend_items)//2 + len(legend_items)%2))))
fig.legend(handles=legend_items, loc="center", ncol=2, frameon=False)
plt.axis("off")
plt.tight_layout()
plt.savefig(OUTPUT_PATH / "mega_legend.png", dpi=200, bbox_inches="tight")
plt.close()

print("✔ Mega plot saved:",
      OUTPUT_PATH / "mega_all_models_all_horizons.png",
      OUTPUT_PATH / "mega_all_models_all_horizons_last90.png",
      OUTPUT_PATH / "mega_legend.png",
      OUTPUT_PATH / "mega_all_models_all_horizons.csv",
      OUTPUT_PATH / "mega_color_map.csv")

# ============================================================
# 18) MEGA PLOT — ALL MODELS & ALL HORIZONS IN ONE PNG
#     (Actual + 9 models × 3 horizons, unique colors + legend)
# ============================================================
import matplotlib as mpl
from matplotlib.lines import Line2D

# Helper: long color palette (>= 28 distinct colors)
def long_palette(n_colors=60):
    cms = [plt.cm.tab20, plt.cm.tab20b, plt.cm.tab20c,
           plt.cm.Set3, plt.cm.Dark2, plt.cm.Paired, plt.cm.Accent]
    colors = []
    for cm in cms:
        # some ListedColormaps have .colors; fall back to sampling
        try:
            arr = cm.colors
            colors.extend(arr if isinstance(arr, list) else list(arr))
        except Exception:
            colors.extend([cm(i/(cm.N-1)) for i in range(cm.N)])
        if len(colors) >= n_colors:
            break
    # trim / pad
    if len(colors) < n_colors:
        more = [plt.cm.nipy_spectral(i/n_colors) for i in range(n_colors-len(colors))]
        colors.extend(more)
    return colors[:n_colors]

# Helper: align a prediction series onto a target date index
def align_to_dates(target_dates, pred_dates, pred_values):
    s = pd.Series(pred_values, index=pd.to_datetime(pred_dates))
    s = s.reindex(pd.to_datetime(target_dates))  # align by intersection
    return s.values

# Build everything to plot
mega_lines   = []   # handles for legend
mega_labels  = []   # labels for legend

# Big figure with a dedicated legend panel inside the PNG
fig = plt.figure(figsize=(20, 8))
gs  = fig.add_gridspec(1, 2, width_ratios=[3.8, 2.2])
ax  = fig.add_subplot(gs[0, 0])
axL = fig.add_subplot(gs[0, 1])
axL.axis("off")

# 1) Plot ACTUAL once (union of all horizon dates)
#    We'll just take the widest date span among horizons and plot “actual”.
all_date_segments = []
for h, data in data_by_h.items():
    all_date_segments.append(pd.to_datetime(data["dates_test"]) + pd.to_timedelta(h, "D"))
dates_union = pd.DatetimeIndex(sorted(pd.unique(np.concatenate([d.values for d in all_date_segments]))))
actual_union = supervised.set_index(DATE_COL)[TARGET].reindex(dates_union)

line_actual, = ax.plot(dates_union, actual_union.values, color="k", lw=3, label="Actual")
mega_lines.append(line_actual); mega_labels.append("Actual")

# 2) Predictions: define the 9 models in the order you want to appear
model_order = [
    "LinearRegression", "LassoCV", "RandomForest", "GBDT",
    "SARIMAX", "TabNet", "TabNet-MT", "LSTM-MT", "TabTransformer"
]

# Colors: one unique color PER (model, horizon)
total_needed = 1 + len(model_order)*len(HORIZONS)  # +1 for actual
palette = long_palette(total_needed + 5)
color_iter = iter(palette[1:])  # reserve black for Actual

# Retrieve & plot each model-horizon
for model_name in model_order:
    for h in HORIZONS:
        # consistent target axis for this horizon
        dates_h = pd.to_datetime(data_by_h[h]["dates_test"]) + pd.to_timedelta(h, "D")
        y = None

        if model_name in ("LinearRegression", "LassoCV", "RandomForest", "GBDT", "SARIMAX"):
            # baselines & SARIMAX are in pred_store[h][name]
            if model_name == "SARIMAX":
                key = "SARIMAX"
            else:
                key = model_name
            if h in pred_store and key in pred_store[h]:
                y = np.asarray(pred_store[h][key])
        elif model_name == "TabNet":
            y = np.asarray(tabnet_preds.get(h, None))
        elif model_name == "TabNet-MT":
            # Yhat_mt present for all horizons; align by column order of HORIZONS
            j = HORIZONS.index(h)
            if "Yhat_mt" in globals():
                y = np.asarray(Yhat_mt[:, j])
        elif model_name == "LSTM-MT":
            if h in lstm_mt_preds:
                y = align_to_dates(dates_h, lstm_mt_preds[h]["dates"], lstm_mt_preds[h]["y_pred"])
        elif model_name == "TabTransformer":
            if h in tt_preds:
                y = align_to_dates(dates_h, tt_preds[h]["dates"], tt_preds[h]["y_pred"])

        # Skip if missing (safeguard)
        if y is None or len(y) != len(dates_h):
            continue

        c = next(color_iter)
        # thinner lines so 27 fit visually; keep readable
        line, = ax.plot(dates_h, y, lw=1.7, color=c,
                        label=f"{model_name} t+{h}")
        mega_lines.append(line)
        mega_labels.append(f"{model_name} t+{h}")

# Axes cosmetics
ax.set_title("ALL models × horizons on one timeline (Actual + 9 models × 3 horizons)")
ax.set_xlabel("Date")
ax.set_ylabel("Violations (tickets)")
ax.grid(True, alpha=0.3)

# Build the legend *inside the same PNG* in the right panel
# Use many rows (one per entry) to keep it readable.
axL.legend(mega_lines, mega_labels, loc="upper left",
           frameon=False, fontsize=11, ncol=1, handlelength=3)

fig.tight_layout()
fig.savefig(OUTPUT_PATH / "mega_all_models_one_plot.png", dpi=200)
plt.close(fig)

print("✓ Mega comparison saved:", OUTPUT_PATH / "mega_all_models_one_plot.png")

# ============================================================
# 19) MEGA PLOTS FOR SPECIFIC WINDOWS
#     (Apr 01–08, 2025) and (May 22–Jun 01, 2025)
# ============================================================
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.gridspec import GridSpec

# --- helpers (no-ops if already defined above) ---
try:
    long_palette
except NameError:
    def long_palette(n_colors=60):
        cms = [plt.cm.tab20, plt.cm.tab20b, plt.cm.tab20c,
               plt.cm.Set3, plt.cm.Dark2, plt.cm.Paired, plt.cm.Accent]
        colors = []
        for cm in cms:
            try:
                arr = cm.colors
                colors.extend(arr if isinstance(arr, list) else list(arr))
            except Exception:
                colors.extend([cm(i/(cm.N-1)) for i in range(cm.N)])
            if len(colors) >= n_colors:
                break
        if len(colors) < n_colors:
            colors.extend([plt.cm.nipy_spectral(i/n_colors)
                           for i in range(n_colors-len(colors))])
        return colors[:n_colors]

try:
    align_to_dates
except NameError:
    def align_to_dates(target_dates, pred_dates, pred_values):
        s = pd.Series(pred_values, index=pd.to_datetime(pred_dates))
        s = s.reindex(pd.to_datetime(target_dates))
        return s.values

model_order = [
    "LinearRegression", "LassoCV", "RandomForest", "GBDT",
    "SARIMAX", "TabNet", "TabNet-MT", "LSTM-MT", "TabTransformer"
]

def make_mega_plot_for_range(start_date, end_date, out_name):
    """Make one giant comparison PNG for a given inclusive date range."""
    start = pd.to_datetime(start_date)
    end   = pd.to_datetime(end_date)
    win_idx = pd.date_range(start, end, freq="D")

    # ------------------ figure + legend panel ------------------
    fig = plt.figure(figsize=(20, 8))
    gs  = fig.add_gridspec(1, 2, width_ratios=[3.8, 2.2])
    ax  = fig.add_subplot(gs[0, 0])
    axL = fig.add_subplot(gs[0, 1]); axL.axis("off")

    # ------------------ ACTUAL ------------------
    actual = supervised.set_index(DATE_COL)[TARGET].reindex(win_idx)
    line_actual, = ax.plot(win_idx, actual.values, color="k", lw=3, label="Actual")

    # ------------------ COLORS ------------------
    total_needed = 1 + len(model_order)*len(HORIZONS)  # +1 for actual
    palette = long_palette(total_needed + 5)
    color_map = {}
    k = 1  # 0 reserved for black actual
    for m in model_order:
        for h in HORIZONS:
            color_map[(m, h)] = palette[k]
            k += 1

    legend_lines = [line_actual]
    legend_labels = ["Actual"]

    # iterate models × horizons
    for model_name in model_order:
        for h in HORIZONS:
            # target dates for this horizon, then crop to window
            dates_h_full = pd.to_datetime(data_by_h[h]["dates_test"]) + pd.to_timedelta(h, "D")
            # get model predictions + align
            y = None
            if model_name in ("LinearRegression", "LassoCV", "RandomForest", "GBDT", "SARIMAX"):
                key = "SARIMAX" if model_name == "SARIMAX" else model_name
                if h in pred_store and key in pred_store[h]:
                    s = pd.Series(np.asarray(pred_store[h][key]), index=dates_h_full)
                    s = s.reindex(win_idx)
                    y = s.values
            elif model_name == "TabNet":
                if h in tabnet_preds:
                    s = pd.Series(np.asarray(tabnet_preds[h]), index=dates_h_full)
                    s = s.reindex(win_idx); y = s.values
            elif model_name == "TabNet-MT":
                if "Yhat_mt" in globals():
                    j = HORIZONS.index(h)
                    s = pd.Series(np.asarray(Yhat_mt[:, j]), index=dates_h_full)
                    s = s.reindex(win_idx); y = s.values
            elif model_name == "LSTM-MT":
                if h in lstm_mt_preds:
                    y = align_to_dates(win_idx, lstm_mt_preds[h]["dates"], lstm_mt_preds[h]["y_pred"])
            elif model_name == "TabTransformer":
                if h in tt_preds:
                    y = align_to_dates(win_idx, tt_preds[h]["dates"], tt_preds[h]["y_pred"])

            if y is None or np.all(np.isnan(y)):
                continue  # skip if missing for this window

            line, = ax.plot(win_idx, y, lw=1.7, color=color_map[(model_name, h)],
                            label=f"{model_name} t+{h}")
            legend_lines.append(line)
            legend_labels.append(f"{model_name} t+{h}")

    ax.set_title(f"ALL models × horizons — {start.date()} to {end.date()}")
    ax.set_xlabel("Date"); ax.set_ylabel("Violations (tickets)")
    ax.grid(True, alpha=0.3)

    # Legend printed INSIDE the same PNG
    axL.legend(legend_lines, legend_labels, loc="upper left",
               frameon=False, fontsize=11, ncol=1, handlelength=3)

    fig.tight_layout()
    fig.savefig(OUTPUT_PATH / out_name, dpi=200)
    plt.close(fig)
    print("✓ Window mega plot saved:", OUTPUT_PATH / out_name)

# ---- Generate both requested windows ----
make_mega_plot_for_range("2025-04-01", "2025-04-08",
                         "mega_all_models_2025-04-01_to_2025-04-08.png")
make_mega_plot_for_range("2025-05-22", "2025-06-01",
                         "mega_all_models_2025-05-22_to_2025-06-01.png")