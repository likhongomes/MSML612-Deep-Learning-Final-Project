# -*- coding: utf-8 -*-
"""tabnet_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6qtOj5SbrNd4x4ly-Pz5DDAAAal-XHn
"""

#importing necessary libraries
from pathlib import Path
import numpy as np, pandas as pd, matplotlib.pyplot as plt

#set true if running in colab
COLAB = True
if COLAB:
    from google.colab import drive
    drive.mount('/content/drive')

#MODIFY to adjust root and output folders
#set parent project folder
GDRIVE_ROOT           = Path("/content/drive/MyDrive/MSML612 Project")
#enter folder containing training script output
GDRIVE_OUTPUT_DIRNAME = "Output_tabnet"

#build paths
OUT = GDRIVE_ROOT / GDRIVE_OUTPUT_DIRNAME
EVAL_OUT = OUT / "Eval_Notebook_Artifacts"
EVAL_OUT.mkdir(parents=True, exist_ok=True)

print("Reading artifacts from:", OUT)
print("Saving NEW evaluation figures/tables to:", EVAL_OUT)

HORIZONS = [1, 3, 7]

#setting plotting params
plt.rcParams["figure.figsize"] = (10, 5)
plt.rcParams["axes.grid"] = True

#metric functions
def rmse(y, yhat):
    from sklearn.metrics import mean_squared_error
    return np.sqrt(mean_squared_error(y, yhat))

def mape(y, yhat, eps=1e-8):
    y, yhat = np.asarray(y), np.asarray(yhat)
    return np.mean(np.abs((y - yhat) / (np.abs(y) + eps))) * 100

def smape(y, yhat, eps=1e-8):
    y, yhat = np.asarray(y), np.asarray(yhat)
    return 100 * np.mean(2*np.abs(yhat - y) / (np.abs(y) + np.abs(yhat) + eps))

#load the master leaderboard from the training script
metrics_path = OUT / "metrics_everything.csv"
assert metrics_path.exists(), f"Missing {metrics_path}. Run the training script first."
dfm = pd.read_csv(metrics_path)

dfm["Model"] = dfm["Model"].replace({"SARIMAX": "SARIMAX(1,1,1)(1,1,1,7)"})

#sort and disply preview
dfm = dfm.sort_values(["Horizon", "MAE (tickets)"])
display(dfm.head(15))

#winners per horizon
winners = dfm.loc[dfm.groupby("Horizon")["MAE (tickets)"].idxmin()].reset_index(drop=True)
winners = winners.rename(columns={"MAE (tickets)":"MAE", "RMSE (tickets)":"RMSE", "MAPE (%)":"MAPE"})
display(winners)

#saving the winners table
winners.to_csv(EVAL_OUT / "winners_per_horizon.csv", index=False)
print("Saved:", EVAL_OUT / "winners_per_horizon.csv")

#plot for MAE by model across horizons
pivot_mae = dfm.pivot_table(index="Model", columns="Horizon", values="MAE (tickets)")
ax = pivot_mae.plot(kind="bar")
ax.set_title("MAE by Model across Horizons")
ax.set_ylabel("MAE (tickets)")
ax.set_xlabel("Model")
plt.tight_layout()
plt.savefig(EVAL_OUT / "mae_by_model_across_horizons.png", dpi=200)
plt.show()
print("Saved:", EVAL_OUT / "mae_by_model_across_horizons.png")

#setting baseline models
baseline_set = {
    "LinearRegression", "LassoCV", "RandomForest", "GBDT", "SARIMAX(1,1,1)(1,1,1,7)"
}

rows = []
for h in HORIZONS:
    sub = dfm[dfm["Horizon"] == h]
    sub_base = sub[sub["Model"].isin(baseline_set)].sort_values("MAE (tickets)")
    sub_all  = sub.sort_values("MAE (tickets)")

    if len(sub_base) == 0 or len(sub_all) == 0:
        continue

    best_base_name = sub_base.iloc[0]["Model"]
    best_base_mae  = sub_base.iloc[0]["MAE (tickets)"]

    best_all_name  = sub_all.iloc[0]["Model"]
    best_all_mae   = sub_all.iloc[0]["MAE (tickets)"]

    abs_gain = best_base_mae - best_all_mae
    pct_gain = (abs_gain / best_base_mae) * 100 if best_base_mae > 0 else np.nan

    rows.append({
        "Horizon": h,
        "Best Baseline": best_base_name,
        "Best Baseline MAE": round(best_base_mae, 2),
        "Best Overall": best_all_name,
        "Best Overall MAE": round(best_all_mae, 2),
        "Abs Improvement (tickets)": round(abs_gain, 2),
        "% Improvement vs Baseline": round(pct_gain, 2)
    })

improve = pd.DataFrame(rows)
display(improve)
improve.to_csv(EVAL_OUT / "improvement_over_best_baseline.csv", index=False)
print("Saved:", EVAL_OUT / "improvement_over_best_baseline.csv")

#percent improvement by horizon
plt.figure()
plt.bar([f"t+{h}" for h in improve["Horizon"]], improve["% Improvement vs Baseline"])
plt.title("% Improvement vs Best Baseline (by Horizon)")
plt.ylabel("% improvement (MAE)")
plt.tight_layout()
plt.savefig(EVAL_OUT / "pct_improvement_vs_baseline.png", dpi=200)
plt.show()
print("Saved:", EVAL_OUT / "pct_improvement_vs_baseline.png")

#k worst days
K = 10
rows = []

for h in HORIZONS:
    best_row = dfm[dfm["Horizon"] == h].sort_values("MAE (tickets)").iloc[0]
    best_model = best_row["Model"]
    preds_path = OUT / f"all_model_preds_t+{h}.csv"
    dfp = pd.read_csv(preds_path, parse_dates=["date"]).set_index("date")
    assert best_model in dfp.columns, f"{best_model} not present in {preds_path}"

    tmp = dfp[["Actual", best_model]].dropna().copy()
    tmp["abs_err"] = (tmp["Actual"] - tmp[best_model]).abs()
    worst = tmp.sort_values("abs_err", ascending=False).head(K)
    worst = worst.assign(horizon=h, model=best_model)
    rows.append(worst.reset_index())

worst_k = pd.concat(rows).rename(columns={"index":"date"})
display(worst_k)
worst_k.to_csv(EVAL_OUT / "topK_worst_days_best_overall.csv", index=False)
print("Saved:", EVAL_OUT / "topK_worst_days_best_overall.csv")

#calibration scatterplot for best overall model per horison
for h in HORIZONS:
    best_row = dfm[dfm["Horizon"] == h].sort_values("MAE (tickets)").iloc[0]
    best_model = best_row["Model"]
    preds_path = OUT / f"all_model_preds_t+{h}.csv"
    dfp = pd.read_csv(preds_path, parse_dates=["date"]).set_index("date")
    assert best_model in dfp.columns, f"{best_model} not present in {preds_path}"

    y = dfp["Actual"].values
    yhat = dfp[best_model].values

    plt.figure()
    plt.scatter(y, yhat, s=10, alpha=0.6)
    ax = plt.gca()
    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]
    plt.plot(lims, lims, '--')
    plt.title(f"Calibration — {best_model} (t+{h})")
    plt.xlabel("Actual (tickets)")
    plt.ylabel("Predicted (tickets)")
    plt.tight_layout()
    fname = EVAL_OUT / f"calibration_{best_model.replace('/','-')}_t+{h}.png"
    plt.savefig(fname, dpi=200)
    plt.show()
    print("Saved:", fname)

cov_path = OUT / "conformal_coverage_tabnet.csv"
if cov_path.exists():
    cov = pd.read_csv(cov_path)
    cov["Abs Error to 90%"] = (cov["Empirical"] - cov["Nominal"]).abs()
    display(cov)

    plt.figure()
    plt.bar([f"t+{h}" for h in cov["Horizon"]], cov["Empirical"])
    plt.axhline(0.90, linestyle="--")
    plt.ylim(0.7,1.0)
    plt.title("Conformal Coverage (TabNet) — target 0.90")
    plt.tight_layout()
    plt.savefig(EVAL_OUT / "conformal_coverage_bar.png", dpi=200)
    plt.show()
    print("Saved:", EVAL_OUT / "conformal_coverage_bar.png")
else:
    print("Note:", cov_path, "not found (skip conformal coverage plot).")

#concise evaluation summary csv
summary = winners[["Horizon","Model","MAE","RMSE","MAPE"]].rename(
    columns={"Model":"Best Overall", "MAE":"Best MAE", "RMSE":"Best RMSE", "MAPE":"Best MAPE"}
)

summary = summary.merge(
    improve[["Horizon","Best Baseline","Best Baseline MAE","Abs Improvement (tickets)","% Improvement vs Baseline"]],
    on="Horizon", how="left"
)

if (OUT / "conformal_coverage_tabnet.csv").exists():
    cov = pd.read_csv(OUT / "conformal_coverage_tabnet.csv")[["Horizon","Nominal","Empirical"]]
    summary = summary.merge(cov, on="Horizon", how="left")

if (OUT / "tabnet_rolling_origin_cv.csv").exists():
    ro = pd.read_csv(OUT / "tabnet_rolling_origin_cv.csv").groupby("Horizon").agg(
        RO_CV_MAE_mean=("MAE","mean"), RO_CV_MAE_std=("MAE","std")
    ).reset_index()
    summary = summary.merge(ro, on="Horizon", how="left")

display(summary)
summary.to_csv(EVAL_OUT / "evaluation_summary_for_report.csv", index=False)
print("Saved:", EVAL_OUT / "evaluation_summary_for_report.csv")

#final paths
print("\nNew evaluation artifacts saved to:", EVAL_OUT)
print("Upstream training artifacts were read from:", OUT)